{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "control-flags-section",
   "metadata": {},
   "source": [
    "## 1. Control Flags\n",
    "Set the desired part(s) to `True` to run. You can run more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "control-flags-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_COMPREHENSIVE_COMPARISON = True  # Abhinav's Part\n",
    "RUN_CROSS_LINGUAL_EVALUATION = True  # Max's Part\n",
    "RUN_DEPRESSION_DETECTION = True      # Patricia's Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup-section",
   "metadata": {},
   "source": [
    "## 2. Imports and System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "imports-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Setup device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## 3. Unified Configuration (CFG)\n",
    "This class holds all parameters for all parts of the notebook. **You must update the file paths to match your system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in a Google Colab environment. Drive mounting skipped.\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # --- General Parameters ---\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # === Paths (UPDATE THESE) ===\n",
    "    # For Comprehensive Comparison & Cross-Lingual Eval\n",
    "    TESS_DATASET_PATH = \"C:/Users/ramir/Downloads/dataverse_files\"\n",
    "    RAVDESS_DATASET_PATH = \"C:/Users/ramir/Downloads/archive (3)/audio_speech_actors_01-24\"\n",
    "    SAVEE_DATASET_PATH = \"C:/Users/ramir/Downloads/archive (2)\"\n",
    "    CREMA_DATASET_PATH = \"C:/Users/ramir/Downloads/CREMA-D/AudioWAV\"\n",
    "    EMODB_DATASET_PATH = \"C:/Users/ramir/Downloads/German-dataset/wav\" \n",
    "    ASED_DATASET_PATH = \"C:/Users/ramir/Downloads/ASED_V1-main/ASED_V1-main\" \n",
    "    \n",
    "    # For Depression Detection\n",
    "    # Assumes your Google Drive is mounted at /content/drive\n",
    "    DAIC_WOZ_DATA_PATH = r\"C:/Users/ramir/Downloads/data-20250617T173532Z-1-002/data\"\n",
    "    DEPRESSION_PRETRAINED_MODEL_PATH = \"C:/Users/ramir/Downloads/best_model_mfcc_attention.pth\"\n",
    "    \n",
    "    # For Cross-Lingual Evaluation\n",
    "    CROSS_LINGUAL_PRETRAINED_MODEL_PATH = \"C:/Users/ramir/Downloads/best_model_mfcc_attention.pth\"\n",
    "\n",
    "    # --- Feature Parameters ---\n",
    "    # For  Comparison (Abhinav)\n",
    "    SR_COMP = 22050\n",
    "    FIXED_TIME_STEPS_COMP = 260 \n",
    "    HOP_LENGTH_UNIFIED_COMP = 256\n",
    "    N_MFCC_COMP = 40\n",
    "    N_FFT_MFCC_COMP = 1024\n",
    "    N_MELS_COMP = 128\n",
    "    N_FFT_MEL_COMP = 2048\n",
    "    BASE_FEATURES_PATH = \"./preprocessed_features_comparison\"\n",
    "\n",
    "    # For Cross-Lingual (Max)\n",
    "    SR_CROSS = 22050\n",
    "    FIXED_TIME_STEPS_CROSS = 250\n",
    "    N_MFCC_CROSS = 40\n",
    "    N_FFT_MFCC_CROSS = 1024\n",
    "    HOP_LENGTH_MFCC_CROSS = 512\n",
    "    \n",
    "    # For Depression Detection (Patricia)\n",
    "    SR_DEP = 44100\n",
    "    DURATION_DEP = 2.5\n",
    "    N_MFCC_DEP = 40\n",
    "    N_MELS_DEP = 64\n",
    "    N_FFT_DEP = 2048\n",
    "    HOP_LENGTH_DEP = 512\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    # For Comprehensive Comparison (Abhinav)\n",
    "    BATCH_SIZE_COMP = 32\n",
    "    NUM_EPOCHS_COMP = 15 # replace with 100 to match report performance\n",
    "    LEARNING_RATE_COMP = 0.001\n",
    "\n",
    "    # For Cross-Lingual (Max)\n",
    "    BATCH_SIZE_CROSS = 32\n",
    "    NUM_EPOCHS_CROSS = 40\n",
    "    LEARNING_RATE_CROSS = 0.001\n",
    "    \n",
    "    # For Depression Detection (Patricia)\n",
    "    BATCH_SIZE_DEP = 32\n",
    "    NUM_EPOCHS_DEP = 150\n",
    "    LEARNING_RATE_DEP = 0.002\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mappings-section",
   "metadata": {},
   "source": [
    "## 4. Emotion & Dataset Mappings\n",
    "Defines the canonical emotion-to-integer mapping and dataset-specific parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mappings-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANONICAL_EMOTION_TO_INT = {'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3, 'angry': 4, 'fear': 5, 'disgust': 6, 'surprise': 7}\n",
    "INT_TO_CANONICAL_EMOTION = {v: k for k, v in CANONICAL_EMOTION_TO_INT.items()}\n",
    "NUM_CLASSES = len(CANONICAL_EMOTION_TO_INT)\n",
    "\n",
    "TESS_FILENAME_TO_CANONICAL_STRING = {'ps': 'surprise', 'sad': 'sad', 'angry': 'angry', 'disgust': 'disgust', 'fear': 'fear', 'happy': 'happy', 'neutral': 'neutral'}\n",
    "\n",
    "RAVDESS_CODE_TO_CANONICAL_STRING = {'01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad', '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'}\n",
    "\n",
    "SAVEE_FILENAME_PREFIX_TO_CANONICAL_STRING = {'a': 'angry', 'd': 'disgust', 'f': 'fear', 'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprise'}\n",
    "\n",
    "CREMA_FILENAME_TO_CANONICAL_STRING = {'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fear', 'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'}\n",
    "\n",
    "EMODB_FILENAME_TO_CANONICAL_STRING = {'W': 'angry', 'L': 'bored', 'E': 'disgust', 'A': 'fear', 'F': 'happy', 'T': 'sad', 'N': 'neutral'}\n",
    "\n",
    "ASED_FILENAME_TO_CANONICAL_STRING = {'n1': 'neutral', 'f2': 'fear', 'h3': 'happy', 's4': 'sad', 'a5': 'angry'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-section",
   "metadata": {},
   "source": [
    "## 5. Data Loading Functions\n",
    "Functions to load audio file paths and labels from various datasets into Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tess_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for dirname, _, filenames in os.walk(dataset_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.wav'):\n",
    "                try:\n",
    "                    emotion_str = TESS_FILENAME_TO_CANONICAL_STRING.get(filename.split('_')[2].split('.')[0].lower())\n",
    "                    if emotion_str:\n",
    "                        data.append({\"path\": os.path.join(dirname, filename), \"emotion_str\": emotion_str, \"source\": \"TESS\"})\n",
    "                except IndexError: continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_ravdess_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for actor_dir in os.listdir(dataset_path):\n",
    "        actor_path = os.path.join(dataset_path, actor_dir)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for filename in os.listdir(actor_path):\n",
    "                if filename.endswith('.wav'):\n",
    "                    parts = filename.split('.')[0].split('-')\n",
    "                    if len(parts) > 2 and parts[0] == '03' and parts[1] == '01':\n",
    "                        emotion_str = RAVDESS_CODE_TO_CANONICAL_STRING.get(parts[2])\n",
    "                        if emotion_str:\n",
    "                            data.append({\"path\": os.path.join(actor_path, filename), \"emotion_str\": emotion_str, \"source\": \"RAVDESS\"})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_savee_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for dirname, _, filenames in os.walk(dataset_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.wav'):\n",
    "                match = re.match(r\"([a-zA-Z]+)(\\d+).wav\", filename)\n",
    "                if match:\n",
    "                    emotion_prefix = match.group(1).lower()\n",
    "                    emotion_str = SAVEE_FILENAME_PREFIX_TO_CANONICAL_STRING.get(emotion_prefix)\n",
    "                    if emotion_str: data.append({\"path\": os.path.join(dirname, filename), \"emotion_str\": emotion_str, \"source\": \"SAVEE\"})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_crema_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith('.wav'):\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                emotion_code = parts[2]\n",
    "                emotion_str = CREMA_FILENAME_TO_CANONICAL_STRING.get(emotion_code)\n",
    "                if emotion_str: data.append({\"path\": os.path.join(dataset_path, filename), \"emotion_str\": emotion_str, \"source\": \"CREMA-D\"})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_emodb_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith('.wav') and len(filename) > 6:\n",
    "            emotion_code = filename[5].upper()\n",
    "            if emotion_code == 'L': continue # Skip 'bored'\n",
    "            emotion_str = EMODB_FILENAME_TO_CANONICAL_STRING.get(emotion_code)\n",
    "            if emotion_str: data.append({\"path\": os.path.join(dataset_path, filename), \"emotion_str\": emotion_str, \"source\": \"EMO-DB\"})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_ased_data(dataset_path):\n",
    "    data = []\n",
    "    if not os.path.exists(dataset_path): return pd.DataFrame(data)\n",
    "    for emotion_folder in os.listdir(dataset_path):\n",
    "        emotion_path = os.path.join(dataset_path, emotion_folder)\n",
    "        if os.path.isdir(emotion_path):\n",
    "            for file in os.listdir(emotion_path):\n",
    "                if file.endswith('.wav'):\n",
    "                    try:\n",
    "                        emotion_code = file.split('-')[0]\n",
    "                        emotion_str = ASED_FILENAME_TO_CANONICAL_STRING.get(emotion_code)\n",
    "                        if emotion_str: data.append({'path': os.path.join(emotion_path, file), 'emotion_str': emotion_str, 'source': 'ASED'})\n",
    "                    except IndexError: continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_daic_woz_data(dataset_path):\n",
    "    data = []\n",
    "    try:\n",
    "        depression_info_train = os.path.join(dataset_path, \"train_split_Depression_AVEC2017.csv\")\n",
    "        depression_info_test = os.path.join(dataset_path, \"full_test_split.csv\")\n",
    "        df_train = pd.read_csv(depression_info_train)\n",
    "        df_test = pd.read_csv(depression_info_test)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: DAIC-WOZ CSV files not found in {dataset_path}. Cannot load depression data.\")\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    # Combine info from both splits\n",
    "    df_train.rename(columns={'PHQ8_Score': 'phq8'}, inplace=True)\n",
    "    df_test.rename(columns={'PHQ_Score': 'phq8'}, inplace=True)\n",
    "    df_info = pd.concat([\n",
    "        df_train[['Participant_ID', 'phq8']],\n",
    "        df_test[['Participant_ID', 'phq8']]\n",
    "    ], ignore_index=True).set_index('Participant_ID')\n",
    "\n",
    "    if os.path.isdir(dataset_path):\n",
    "        for file in os.listdir(dataset_path):\n",
    "            if file.endswith('.wav'):\n",
    "                try:\n",
    "                    participant_id = int(file.split('_')[0])\n",
    "                    if participant_id in df_info.index:\n",
    "                        score = df_info.loc[participant_id, 'phq8']\n",
    "                        file_path = os.path.join(dataset_path, file)\n",
    "                        data.append({'path': file_path, 'emotion': score})\n",
    "                except (ValueError, KeyError):\n",
    "                    continue\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-extraction-section",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction Functions\n",
    "Functions to extract MFCC and Log-Mel Spectrogram features from audio files. These are flexible to handle different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-extraction-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, feature_type, sr, n_mfcc=None, n_mels=None, n_fft=None, hop_length=None, duration=None, fixed_timesteps=None):\n",
    "    \"\"\"A unified feature extraction function.\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        if duration:\n",
    "            audio, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "            target_len = int(sr * duration)\n",
    "            if len(audio) < target_len:\n",
    "                audio = np.pad(audio, (0, target_len - len(audio)), 'constant')\n",
    "            else:\n",
    "                audio = audio[:target_len]\n",
    "        else:\n",
    "            audio, _ = librosa.load(file_path, sr=sr, mono=True)\n",
    "        \n",
    "        if len(audio) < 100: return None\n",
    "\n",
    "        # Extract features\n",
    "        if feature_type == 'mfcc':\n",
    "            features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        elif feature_type == 'melspec':\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "            features = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid feature type specified.\")\n",
    "        \n",
    "        features = features.T\n",
    "\n",
    "        \n",
    "        if fixed_timesteps:\n",
    "            if features.shape[0] > fixed_timesteps:\n",
    "                features = features[:fixed_timesteps, :]\n",
    "            elif features.shape[0] < fixed_timesteps:\n",
    "                features = np.pad(features, ((0, fixed_timesteps - features.shape[0]), (0, 0)), mode='constant')\n",
    "        \n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_and_save_features_comp(df, feature_type, feature_params, features_dir, desc_prefix=\"\"):\n",
    "    \"\"\"Pre-computes and saves features for the comprehensive comparison part.\"\"\"\n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "    new_paths = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Pre-processing {desc_prefix} ({feature_type})\"):\n",
    "        original_path = row['path']\n",
    "        feature_filename = f\"{os.path.basename(original_path).replace('.wav', '')}_{index}.npy\"\n",
    "        feature_path = os.path.join(features_dir, feature_filename)\n",
    "        \n",
    "        if not os.path.exists(feature_path):\n",
    "            params = {\n",
    "                'file_path': original_path,\n",
    "                'feature_type': 'mfcc' if feature_type == 'MFCC' else 'melspec',\n",
    "                'sr': feature_params['sr'],\n",
    "                'n_fft': feature_params['n_fft'],\n",
    "                'hop_length': feature_params['hop_length'],\n",
    "                'fixed_timesteps': feature_params['fixed_length']\n",
    "            }\n",
    "            if feature_type == 'MFCC':\n",
    "                params['n_mfcc'] = feature_params['n_mfcc']\n",
    "            else:\n",
    "                params['n_mels'] = feature_params['n_mels']\n",
    "\n",
    "            features = extract_features(**params)\n",
    "            \n",
    "            if features is not None:\n",
    "                np.save(feature_path, features)\n",
    "            else:\n",
    "                feature_path = None\n",
    "        \n",
    "        new_paths.append(feature_path)\n",
    "        \n",
    "    df['feature_path'] = new_paths\n",
    "    return df.dropna(subset=['feature_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-collate-section",
   "metadata": {},
   "source": [
    "## 7. Dataset and Collate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-collate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_params):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_params = feature_params\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        file_path = row['path']\n",
    "        label = row['emotion']\n",
    "        current_params = self.feature_params.copy()\n",
    "        current_params['file_path'] = file_path\n",
    "        features = extract_features(**current_params)\n",
    "        \n",
    "        if features is None:\n",
    "            n_feat = self.feature_params.get('n_mfcc') or self.feature_params.get('n_mels')\n",
    "            n_steps = self.feature_params.get('fixed_timesteps') or 1\n",
    "            shape = (n_steps, n_feat)\n",
    "            features = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if np.std(features) > 1e-8:\n",
    "            features = (features - np.mean(features)) / np.std(features)\n",
    "            \n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label) # Dtype inferred\n",
    "\n",
    "# --- For Pre-Computed Features (Abhinav) ---\n",
    "class PrecomputedDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        print(f\"Loading {len(self.dataframe)} pre-computed features into RAM...\")\n",
    "        for _, row in tqdm(self.dataframe.iterrows(), total=len(self.dataframe)):\n",
    "            feature_data = np.load(row['feature_path'])\n",
    "            if np.std(feature_data) > 1e-8:\n",
    "                feature_data = (feature_data - np.mean(feature_data)) / np.std(feature_data)\n",
    "            self.features.append(torch.tensor(feature_data, dtype=torch.float32))\n",
    "            self.labels.append(torch.tensor(row['emotion'], dtype=torch.long))\n",
    "        print(\"...Done loading.\")\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# --- For Depression Regression (Patricia) ---\n",
    "class DepressionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe['values'].tolist(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(dataframe['phq8'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    batch = [(x, y) for x, y in batch if x is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "        \n",
    "    (xx, yy) = zip(*batch)\n",
    "    max_len = max(x.shape[0] for x in xx)\n",
    "\n",
    "    padded_xx = []\n",
    "    for x_item in xx:\n",
    "        num_features = x_item.shape[1]\n",
    "        if x_item.shape[0] < max_len:\n",
    "            padding = torch.zeros((max_len - x_item.shape[0], num_features))\n",
    "            padded_x_item = torch.cat((x_item, padding), dim=0)\n",
    "        else:\n",
    "            padded_x_item = x_item\n",
    "        padded_xx.append(padded_x_item)\n",
    "\n",
    "    xx_pad = torch.stack(padded_xx)\n",
    "    yy = torch.tensor(yy)\n",
    "    return xx_pad, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-architectures-section",
   "metadata": {},
   "source": [
    "## 8. Model Architectures\n",
    "This section contains all model architectures used across the three projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-architectures-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism to weigh the importance of different time steps.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_net = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
    "    def forward(self, x):\n",
    "        energies = self.attention_net(x).squeeze(2)\n",
    "        weights = F.softmax(energies, dim=1)\n",
    "        return x * weights.unsqueeze(2)\n",
    "\n",
    "def create_cnn_block():\n",
    "    \"\"\"Creates a standard 4-layer CNN block used by several models.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "        nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "        nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "        nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "\n",
    "class SpeechEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = create_cnn_block()\n",
    "        lstm_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.lstm1 = nn.LSTM(lstm_in, 32, batch_first=True)\n",
    "        self.attention = Attention(32)\n",
    "        self.lstm2 = nn.LSTM(32, 32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x.unsqueeze(1))\n",
    "        b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        self.lstm1.flatten_parameters()\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        attended_out = self.attention(lstm1_out)\n",
    "        self.lstm2.flatten_parameters()\n",
    "        _, (h_n, _) = self.lstm2(attended_out)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class DepressionModel(nn.Module):\n",
    "    def __init__(self, n_inputs, hidden_dims=[64, 32], dropout_rate=0.3):\n",
    "        super(DepressionModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = n_inputs\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1)) \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "    \n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        weights = torch.where(target >= 10, 10.0, 1.0) \n",
    "        error = pred - target\n",
    "        under_prediction_penalty = torch.where((target >= 10) & (error < 0), 3.0, 1.0)\n",
    "        mse = (pred - target) ** 2\n",
    "        weighted_mse = weights * under_prediction_penalty * mse\n",
    "        \n",
    "        pred_std = pred.std()\n",
    "        diversity_penalty = 0.0\n",
    "        if pred_std < 1.0:\n",
    "            diversity_penalty = (1.0 - pred_std) * 10.0\n",
    "        return torch.mean(weighted_mse) + diversity_penalty\n",
    "\n",
    "class Model_NoAttention(nn.Module):\n",
    "    \"\"\"Original model without the attention mechanism.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.cnn = create_cnn_block(); lstm_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.lstm1 = nn.LSTM(lstm_in, 32, batch_first=True); self.lstm2 = nn.LSTM(32, 32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        lstm1_out, _ = self.lstm1(x); _, (h_n, _) = self.lstm2(lstm1_out)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class Model_SimpleLSTM(nn.Module):\n",
    "    \"\"\"A simpler CNN followed by a single LSTM layer.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.cnn = create_cnn_block(); lstm_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.lstm = nn.LSTM(lstm_in, 64, batch_first=True); self.fc = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        _, (h_n, _) = self.lstm(x); return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class Model_SimpleGRU(nn.Module):\n",
    "    \"\"\"A simpler CNN followed by a single GRU layer.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.cnn = create_cnn_block(); gru_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.gru = nn.GRU(gru_in, 64, batch_first=True); self.fc = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        _, h_n = self.gru(x); return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"Efficient convolution block separating spatial and channel-wise convolutions.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, padding='same'):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, 3, padding=padding, groups=in_ch)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1)\n",
    "    def forward(self, x): return self.pointwise(self.depthwise(x))\n",
    "\n",
    "class Model_LightweightCNN_GRU(nn.Module):\n",
    "    \"\"\"Lightweight CNN with Depthwise Separable Convolutions, followed by a GRU.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            DepthwiseSeparableConv(1, 32), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(2),\n",
    "            DepthwiseSeparableConv(32, 64), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(2))\n",
    "        gru_in = 64 * (feature_dim // 4) if (feature_dim // 4) > 0 else 64\n",
    "        self.gru = nn.GRU(gru_in, 64, batch_first=True); self.fc = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        _, h_n = self.gru(x); return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class Model_Efficient_BiGRU(nn.Module):\n",
    "    \"\"\"Standard CNN followed by two efficient, bidirectional GRU layers with attention.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.cnn = create_cnn_block(); gru_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.gru1 = nn.GRU(gru_in, 24, batch_first=True, bidirectional=True); self.attention = Attention(24 * 2)\n",
    "        self.gru2 = nn.GRU(24 * 2, 24, batch_first=True, bidirectional=True); self.fc = nn.Linear(24 * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        gru1_out, _ = self.gru1(x); attended_out = self.attention(gru1_out)\n",
    "        _, h_n = self.gru2(attended_out); return self.fc(torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1))\n",
    "\n",
    "class Model_BiLSTM_Attention(nn.Module):\n",
    "    \"\"\"Standard CNN with two Bidirectional LSTMs and Attention.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.cnn = create_cnn_block(); lstm_in = 64 * (feature_dim // 16) if (feature_dim // 16) > 0 else 64\n",
    "        self.lstm1 = nn.LSTM(lstm_in, 32, batch_first=True, bidirectional=True); self.attention = Attention(32 * 2)\n",
    "        self.lstm2 = nn.LSTM(32 * 2, 32, batch_first=True, bidirectional=True); self.fc = nn.Linear(32 * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)\n",
    "        lstm1_out, _ = self.lstm1(x); attended_out = self.attention(lstm1_out)\n",
    "        _, (h_n, _) = self.lstm2(attended_out); return self.fc(torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information into the input for Transformer models.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__(); pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class Model_Transformer(nn.Module):\n",
    "    \"\"\"Transformer Encoder model.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__(); self.pos_encoder = PositionalEncoding(feature_dim)\n",
    "        n_head = 8 if feature_dim % 8 == 0 else 4 if feature_dim % 4 == 0 else 2 if feature_dim % 2 == 0 else 1\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=n_head, batch_first=True, dim_feedforward=256)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(self.pos_encoder(x)); return self.fc(x.mean(dim=1))\n",
    "\n",
    "class Model_Hybrid_CNN_Transformer(nn.Module):\n",
    "    \"\"\"Hybrid model combining a light CNN with a Transformer Encoder.\"\"\"\n",
    "    def __init__(self, num_classes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(2))\n",
    "        cnn_out_dim = 32 * (feature_dim // 4)\n",
    "        self.pos_encoder = PositionalEncoding(cnn_out_dim)\n",
    "        n_head = 4 if cnn_out_dim % 4 == 0 else 2 if cnn_out_dim % 2 == 0 else 1\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=cnn_out_dim, nhead=n_head, batch_first=True, dim_feedforward=256)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(cnn_out_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1)); b,c,t,f = x.shape; x = x.permute(0,2,1,3).reshape(b,t,c*f)\n",
    "        x = self.transformer_encoder(self.pos_encoder(x)); return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-eval-section",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation Functions\n",
    "Helper functions for training loops and evaluating model performance for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-eval-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in data_loader:\n",
    "        if inputs is None: continue \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if isinstance(criterion, (nn.MSELoss, WeightedMSELoss)):\n",
    "            labels = labels.to(torch.float32)\n",
    "        else: \n",
    "            labels = labels.to(torch.long)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    return running_loss / len(data_loader.dataset)\n",
    "\n",
    "def evaluate_classification(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            if inputs is None: continue\n",
    "            outputs = model(inputs.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    if not all_labels:\n",
    "        return 0.0\n",
    "    return accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "def evaluate_regression(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_values = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            if inputs is None: continue\n",
    "            outputs = model(inputs.to(device))\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_values.extend(labels.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    r2 = r2_score(true_values, predictions) if np.var(predictions) > 1e-6 else 0.0\n",
    "    \n",
    "    binary_true = (true_values >= 10).astype(int)\n",
    "    binary_pred = (predictions >= 10).astype(int)\n",
    "    binary_acc = accuracy_score(binary_true, binary_pred)\n",
    "    binary_f1 = f1_score(binary_true, binary_pred, zero_division=0)\n",
    "    \n",
    "    metrics = {'mae': mae, 'rmse': rmse, 'r2': r2, 'binary_acc': binary_acc, 'binary_f1': binary_f1}\n",
    "    return predictions, true_values, metrics\n",
    "\n",
    "def plot_depression_results(predictions, true_values):\n",
    "    \"\"\"Visualize the results with 4 subplots.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    # Scatter plot\n",
    "    axes[0, 0].scatter(true_values, predictions, alpha=0.5)\n",
    "    axes[0, 0].plot([0, 24], [0, 24], 'r--', label='Ideal Fit')\n",
    "    axes[0, 0].axhline(y=10, color='green', linestyle='--', alpha=0.5, label='Depression Threshold')\n",
    "    axes[0, 0].axvline(x=10, color='green', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('True PHQ-8')\n",
    "    axes[0, 0].set_ylabel('Predicted PHQ-8')\n",
    "    axes[0, 0].set_title('Predictions vs True Values')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Confusion matrix\n",
    "    binary_true = (true_values >= 10).astype(int)\n",
    "    binary_pred = (predictions >= 10).astype(int)\n",
    "    cm = confusion_matrix(binary_true, binary_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
    "                xticklabels=['Not Depressed', 'Depressed'], yticklabels=['Not Depressed', 'Depressed'])\n",
    "    axes[0, 1].set_xlabel('Predicted Label')\n",
    "    axes[0, 1].set_ylabel('True Label')\n",
    "    axes[0, 1].set_title('Depression Detection (PHQ-8 >= 10)')\n",
    "\n",
    "    # Histograms\n",
    "    axes[1, 0].hist(predictions[binary_true == 0], bins=15, alpha=0.5, label='Not Depressed', color='blue')\n",
    "    axes[1, 0].hist(predictions[binary_true == 1], bins=15, alpha=0.5, label='Depressed', color='red')\n",
    "    axes[1, 0].axvline(x=10, color='black', linestyle='--', label='Threshold')\n",
    "    axes[1, 0].set_xlabel('Predicted PHQ-8')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_title('Prediction Distribution by True Class')\n",
    "\n",
    "    # ROC-like curve\n",
    "    thresholds = np.linspace(min(predictions) - 1, max(predictions) + 1, 100)\n",
    "    tpr, fpr = [], []\n",
    "    for thresh in thresholds:\n",
    "        pred_binary = (predictions >= thresh).astype(int)\n",
    "        tp = ((pred_binary == 1) & (binary_true == 1)).sum()\n",
    "        fp = ((pred_binary == 1) & (binary_true == 0)).sum()\n",
    "        tn = ((pred_binary == 0) & (binary_true == 0)).sum()\n",
    "        fn = ((pred_binary == 0) & (binary_true == 1)).sum()\n",
    "        tpr.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        fpr.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "    axes[1, 1].plot(fpr, tpr)\n",
    "    axes[1, 1].plot([0, 1], [0, 1], 'r--')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title('ROC-like Curve')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution-blocks-section",
   "metadata": {},
   "source": [
    "# 10. EXECUTION BLOCKS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-comparison-code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### RUNNING PART 1: COMPREHENSIVE MODEL & FEATURE COMPARISON (Abhinav's Part) ###\n",
      "================================================================================\n",
      "\n",
      "--- Loading All Dataset Metadata ---\n",
      "Loaded a total of 5200 samples for main training.\n",
      "\n",
      "################################################################################\n",
      "# STARTING EXPERIMENTS FOR FEATURE SET: MFCC\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea193a3abd24cb299067fbcca124217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-processing Main (MFCC):   0%|          | 0/5200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split for MFCC. Train: 4160, Test: 1040\n",
      "Loading 4160 pre-computed features into RAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb9f9a5fd441618abfdc9a759292e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done loading.\n",
      "Loading 1040 pre-computed features into RAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18e59f2ff3a4956ac629cc487ac2d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done loading.\n",
      "\n",
      "========================= Experiment: 1. CNN-LSTM w/ Attention on MFCC =========================\n",
      "Number of trainable parameters: 142,473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c7b78557284bdc82f72b1154e9ee49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 1. CNN-LSTM w/ Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 1. CNN-LSTM w/ Attention ---\n",
      "Time: 98.30s | Best Test Acc: 77.31%\n",
      "\n",
      "========================= Experiment: 2. CNN-LSTM No Attention on MFCC =========================\n",
      "Number of trainable parameters: 141,384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e0ddc0103241f59ca92b5ee784c5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 2. CNN-LSTM No Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 2. CNN-LSTM No Attention ---\n",
      "Time: 91.35s | Best Test Acc: 80.00%\n",
      "\n",
      "========================= Experiment: 3. CNN-Simple LSTM on MFCC =========================\n",
      "Number of trainable parameters: 162,120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3297325e5154ad99c4929b6ce9d6a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 3. CNN-Simple LSTM:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 3. CNN-Simple LSTM ---\n",
      "Time: 82.64s | Best Test Acc: 83.08%\n",
      "\n",
      "========================= Experiment: 4. CNN-Simple GRU on MFCC =========================\n",
      "Number of trainable parameters: 149,704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8909c17c740e4192bdde1bc4fcc180ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 4. CNN-Simple GRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 4. CNN-Simple GRU ---\n",
      "Time: 83.81s | Best Test Acc: 84.13%\n",
      "\n",
      "========================= Experiment: 5. Lightweight CNN-GRU on MFCC =========================\n",
      "Number of trainable parameters: 138,770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc928fe6dceb4120a48b230e0142b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 5. Lightweight CNN-GRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 5. Lightweight CNN-GRU ---\n",
      "Time: 49.20s | Best Test Acc: 83.75%\n",
      "\n",
      "========================= Experiment: 6. Efficient CNN-BiGRU on MFCC =========================\n",
      "Number of trainable parameters: 147,561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7230147bcc37466a8e6cf4aded25f475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 6. Efficient CNN-BiGRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 6. Efficient CNN-BiGRU ---\n",
      "Time: 93.62s | Best Test Acc: 82.31%\n",
      "\n",
      "========================= Experiment: 7. CNN-BiLSTM w/ Attention on MFCC =========================\n",
      "Number of trainable parameters: 183,241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0515ed745e94233b653683de9eca1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 7. CNN-BiLSTM w/ Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 7. CNN-BiLSTM w/ Attention ---\n",
      "Time: 93.74s | Best Test Acc: 77.79%\n",
      "\n",
      "========================= Experiment: 8. Transformer on MFCC =========================\n",
      "Number of trainable parameters: 110,312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cab391eed04d32a63b683920f91294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 8. Transformer:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramir\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 8. Transformer ---\n",
      "Time: 175.25s | Best Test Acc: 75.96%\n",
      "\n",
      "========================= Experiment: 9. Hybrid CNN-Transformer on MFCC =========================\n",
      "Number of trainable parameters: 1,160,616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d914c138804c28af216119c6bb6294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 9. Hybrid CNN-Transformer:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 9. Hybrid CNN-Transformer ---\n",
      "Time: 63.00s | Best Test Acc: 83.65%\n",
      "\n",
      "################################################################################\n",
      "# STARTING EXPERIMENTS FOR FEATURE SET: Log-Mel\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded5092608ee45cf8bef65a5f7191098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-processing Main (Log-Mel):   0%|          | 0/5200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split for Log-Mel. Train: 4160, Test: 1040\n",
      "Loading 4160 pre-computed features into RAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e44fbe68b1644d3b3b94a05b50d4542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done loading.\n",
      "Loading 1040 pre-computed features into RAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabc11fa98914e40aa4273822a0a4470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done loading.\n",
      "\n",
      "========================= Experiment: 1. CNN-LSTM w/ Attention on Log-Mel =========================\n",
      "Number of trainable parameters: 191,625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df72a927f7ab43bbb444130884506437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 1. CNN-LSTM w/ Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 1. CNN-LSTM w/ Attention ---\n",
      "Time: 244.33s | Best Test Acc: 75.19%\n",
      "\n",
      "========================= Experiment: 2. CNN-LSTM No Attention on Log-Mel =========================\n",
      "Number of trainable parameters: 190,536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59ba9961c8426399d7255b3edb080e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 2. CNN-LSTM No Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 2. CNN-LSTM No Attention ---\n",
      "Time: 248.18s | Best Test Acc: 80.48%\n",
      "\n",
      "========================= Experiment: 3. CNN-Simple LSTM on Log-Mel =========================\n",
      "Number of trainable parameters: 260,424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be80113268304bc78a3c3beede08a29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 3. CNN-Simple LSTM:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 3. CNN-Simple LSTM ---\n",
      "Time: 240.92s | Best Test Acc: 82.98%\n",
      "\n",
      "========================= Experiment: 4. CNN-Simple GRU on Log-Mel =========================\n",
      "Number of trainable parameters: 223,432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9d721299bb4f6b95346204c68fe290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 4. CNN-Simple GRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 4. CNN-Simple GRU ---\n",
      "Time: 239.84s | Best Test Acc: 85.48%\n",
      "\n",
      "========================= Experiment: 5. Lightweight CNN-GRU on Log-Mel =========================\n",
      "Number of trainable parameters: 409,106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a036fcc41b443d394a95d300a6c3cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 5. Lightweight CNN-GRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 5. Lightweight CNN-GRU ---\n",
      "Time: 123.93s | Best Test Acc: 84.81%\n",
      "\n",
      "========================= Experiment: 6. Efficient CNN-BiGRU on Log-Mel =========================\n",
      "Number of trainable parameters: 202,857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cf41c2f02c4c2d9e93a6a2eed22c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 6. Efficient CNN-BiGRU:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 6. Efficient CNN-BiGRU ---\n",
      "Time: 247.31s | Best Test Acc: 78.37%\n",
      "\n",
      "========================= Experiment: 7. CNN-BiLSTM w/ Attention on Log-Mel =========================\n",
      "Number of trainable parameters: 281,545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75aea8ccaed40e9bee866f000296a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 7. CNN-BiLSTM w/ Attention:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 7. CNN-BiLSTM w/ Attention ---\n",
      "Time: 239.04s | Best Test Acc: 78.27%\n",
      "\n",
      "========================= Experiment: 8. Transformer on Log-Mel =========================\n",
      "Number of trainable parameters: 530,952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f730af4aae6446c8ebe3c8e888287fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 8. Transformer:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 8. Transformer ---\n",
      "Time: 195.45s | Best Test Acc: 84.71%\n",
      "\n",
      "========================= Experiment: 9. Hybrid CNN-Transformer on Log-Mel =========================\n",
      "Number of trainable parameters: 9,469,224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501adfd6b97347469c9d4e7e1635a17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 9. Hybrid CNN-Transformer:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete for 9. Hybrid CNN-Transformer ---\n",
      "Time: 223.28s | Best Test Acc: 35.19%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- FINAL COMPREHENSIVE RESULTS SUMMARY ---\n",
      "================================================================================\n",
      "Feature Set                 Model Name Parameters  Test Acc (%)\n",
      "    Log-Mel          4. CNN-Simple GRU    223,432     85.480769\n",
      "    Log-Mel     5. Lightweight CNN-GRU    409,106     84.807692\n",
      "    Log-Mel             8. Transformer    530,952     84.711538\n",
      "    Log-Mel         3. CNN-Simple LSTM    260,424     82.980769\n",
      "    Log-Mel   2. CNN-LSTM No Attention    190,536     80.480769\n",
      "    Log-Mel     6. Efficient CNN-BiGRU    202,857     78.365385\n",
      "    Log-Mel 7. CNN-BiLSTM w/ Attention    281,545     78.269231\n",
      "    Log-Mel   1. CNN-LSTM w/ Attention    191,625     75.192308\n",
      "    Log-Mel  9. Hybrid CNN-Transformer  9,469,224     35.192308\n",
      "       MFCC          4. CNN-Simple GRU    149,704     84.134615\n",
      "       MFCC     5. Lightweight CNN-GRU    138,770     83.750000\n",
      "       MFCC  9. Hybrid CNN-Transformer  1,160,616     83.653846\n",
      "       MFCC         3. CNN-Simple LSTM    162,120     83.076923\n",
      "       MFCC     6. Efficient CNN-BiGRU    147,561     82.307692\n",
      "       MFCC   2. CNN-LSTM No Attention    141,384     80.000000\n",
      "       MFCC 7. CNN-BiLSTM w/ Attention    183,241     77.788462\n",
      "       MFCC   1. CNN-LSTM w/ Attention    142,473     77.307692\n",
      "       MFCC             8. Transformer    110,312     75.961538\n",
      "================================================================================\n",
      "Comprehensive Comparison Complete.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if RUN_COMPREHENSIVE_COMPARISON:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### RUNNING PART 1: COMPREHENSIVE MODEL & FEATURE COMPARISON (Abhinav's Part) ###\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"--- Loading All Dataset Metadata ---\")\n",
    "    df_tess = load_tess_data(cfg.TESS_DATASET_PATH)\n",
    "    if df_tess.empty: print(\"Warning: TESS dataset is empty. Check path in CFG.\")\n",
    "    \n",
    "    df_ravdess = load_ravdess_data(cfg.RAVDESS_DATASET_PATH)\n",
    "    if df_ravdess.empty: print(\"Warning: RAVDESS dataset is empty. Check path in CFG.\")\n",
    "    \n",
    "    df_savee = load_savee_data(cfg.SAVEE_DATASET_PATH)\n",
    "    if df_savee.empty: print(\"Warning: SAVEE dataset is empty. Check path in CFG.\")\n",
    "    \n",
    "    df_crema = load_crema_data(cfg.CREMA_DATASET_PATH)\n",
    "    if df_crema.empty: print(\"Warning: CREMA-D dataset is empty. Check path in CFG.\")\n",
    "\n",
    "    df_list = [df for df in [df_tess, df_ravdess, df_savee] if not df.empty] # df_crema add this for extra data \n",
    "    if not df_list:\n",
    "        print(\"\\nFATAL ERROR: No datasets were loaded. All paths might be incorrect in the CFG class.\")\n",
    "        print(\"Skipping the rest of the Comprehensive Comparison part.\")\n",
    "    else:\n",
    "        df_main_meta = pd.concat(df_list, ignore_index=True)\n",
    "        df_main_meta['emotion'] = df_main_meta['emotion_str'].map(CANONICAL_EMOTION_TO_INT)\n",
    "        df_main_meta.dropna(subset=['emotion'], inplace=True)\n",
    "        df_main_meta['emotion'] = df_main_meta['emotion'].astype(int)\n",
    "        print(f\"Loaded a total of {len(df_main_meta)} samples for main training.\")\n",
    "        \n",
    "        feature_sets_to_run = {\n",
    "            \"MFCC\": {\n",
    "                \"feature_dim\": cfg.N_MFCC_COMP,\n",
    "                \"params\": {'sr': cfg.SR_COMP, 'n_mfcc': cfg.N_MFCC_COMP, 'n_fft': cfg.N_FFT_MFCC_COMP, 'hop_length': cfg.HOP_LENGTH_UNIFIED_COMP, 'fixed_length': cfg.FIXED_TIME_STEPS_COMP}\n",
    "            },\n",
    "            \"Log-Mel\": {\n",
    "                \"feature_dim\": cfg.N_MELS_COMP,\n",
    "                \"params\": {'sr': cfg.SR_COMP, 'n_mels': cfg.N_MELS_COMP, 'n_fft': cfg.N_FFT_MEL_COMP, 'hop_length': cfg.HOP_LENGTH_UNIFIED_COMP, 'fixed_length': cfg.FIXED_TIME_STEPS_COMP}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        experiment_models = [\n",
    "            (\"1. CNN-LSTM w/ Attention\", SpeechEmotionModel),\n",
    "            (\"2. CNN-LSTM No Attention\", Model_NoAttention),\n",
    "            (\"3. CNN-Simple LSTM\", Model_SimpleLSTM),\n",
    "            (\"4. CNN-Simple GRU\", Model_SimpleGRU),\n",
    "            (\"5. Lightweight CNN-GRU\", Model_LightweightCNN_GRU),\n",
    "            (\"6. Efficient CNN-BiGRU\", Model_Efficient_BiGRU),\n",
    "            (\"7. CNN-BiLSTM w/ Attention\", Model_BiLSTM_Attention),\n",
    "            (\"8. Transformer\", Model_Transformer),\n",
    "            (\"9. Hybrid CNN-Transformer\", Model_Hybrid_CNN_Transformer)\n",
    "        ]\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for feature_name, feature_config in feature_sets_to_run.items():\n",
    "            print(f\"\\n{'#'*80}\\n# STARTING EXPERIMENTS FOR FEATURE SET: {feature_name}\\n{'#'*80}\")\n",
    "            feature_dir = os.path.join(cfg.BASE_FEATURES_PATH, feature_name.lower())\n",
    "            \n",
    "            df_main = preprocess_and_save_features_comp(\n",
    "                df_main_meta.copy(), \n",
    "                feature_name, \n",
    "                feature_config[\"params\"], \n",
    "                feature_dir, \n",
    "                desc_prefix=\"Main\"\n",
    "            )\n",
    "\n",
    "            train_df, test_df = train_test_split(df_main, test_size=0.2, random_state=cfg.RANDOM_STATE, stratify=df_main['emotion'])\n",
    "            print(f\"\\nData split for {feature_name}. Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "            \n",
    "            train_dataset = PrecomputedDataset(train_df)\n",
    "            test_dataset = PrecomputedDataset(test_df)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE_COMP, shuffle=True, num_workers=0)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE_COMP, shuffle=False, num_workers=0)\n",
    "            for model_name, model_class in experiment_models:\n",
    "                print(f\"\\n{'='*25} Experiment: {model_name} on {feature_name} {'='*25}\")\n",
    "                model = model_class(NUM_CLASSES, feature_config[\"feature_dim\"]).to(DEVICE)\n",
    "                print(f\"Number of trainable parameters: {count_parameters(model):,}\")\n",
    "                \n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE_COMP)\n",
    "                \n",
    "                start_time, best_test_accuracy = time.time(), 0.0\n",
    "                \n",
    "                pbar = tqdm(range(cfg.NUM_EPOCHS_COMP), desc=f\"Training {model_name}\")\n",
    "                for epoch in pbar:\n",
    "                    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "                    epoch_test_accuracy = evaluate_classification(model, test_loader, DEVICE)\n",
    "                    if epoch_test_accuracy > best_test_accuracy:\n",
    "                        best_test_accuracy = epoch_test_accuracy\n",
    "                    pbar.set_description(f\"Training {model_name} (Loss: {train_loss:.4f}, Best Acc: {best_test_accuracy:.2f}%)\")\n",
    "                \n",
    "                total_time = time.time() - start_time\n",
    "                print(f\"--- Training Complete for {model_name} ---\")\n",
    "                print(f\"Time: {total_time:.2f}s | Best Test Acc: {best_test_accuracy:.2f}%\")\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"Feature Set\": feature_name, \"Model Name\": model_name, \"Parameters\": count_parameters(model),\n",
    "                    \"Test Acc (%)\": best_test_accuracy\n",
    "                })\n",
    "\n",
    "        # --- Final Results Summary ---\n",
    "        print(\"\\n\\n\" + \"=\"*80 + \"\\n--- FINAL COMPREHENSIVE RESULTS SUMMARY ---\\n\" + \"=\"*80)\n",
    "        if all_results:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df['Parameters'] = results_df['Parameters'].apply(lambda x: f\"{x:,}\")\n",
    "            results_df = results_df.sort_values(by=[\"Feature Set\", \"Test Acc (%)\"], ascending=[True, False])\n",
    "            pd.set_option('display.max_columns', None); pd.set_option('display.width', 1000)\n",
    "            print(results_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"No results to display.\")\n",
    "        print(\"=\"*80 + \"\\nComprehensive Comparison Complete.\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-lingual-code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### RUNNING PART 2: CROSS-LINGUAL EVALUATION (Max's Part) ###\n",
      "================================================================================\n",
      "\n",
      "--- Loading Cross-Lingual Datasets ---\n",
      "Loaded 2473 ASED samples and 454 EmoDB samples.\n",
      "\n",
      "--- Initializing Models ---\n",
      "Successfully loaded pre-trained model.\n",
      "\n",
      "--- Preparing Data and Starting Training ---\n",
      "\n",
      "--- Training Model: Finetune ASED ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0412f58e93df45a8a8adc768d18f0e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Finetune ASED:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: Finetune EmoDB ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269e573d70a44d22b82637107706e342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Finetune EmoDB:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: Finetune Total ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45f2ddd5e424d0299f3fecc294e0b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Finetune Total:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: Scratch ASED ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a16b1ad79b429e9add811972969d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Scratch ASED:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: Scratch EmoDB ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46840016d6e04198b4eedb2c8d35599d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Scratch EmoDB:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### FINAL CROSS-LINGUAL EVALUATION RESULTS ###\n",
      "================================================================================\n",
      "\n",
      "                       ASED   EmoDB   Total\n",
      "Pre-trained English  17.43%  23.57%  18.38%\n",
      "Finetune ASED        98.18%  30.18%  87.63%\n",
      "Finetune EmoDB       32.63%  90.09%  41.54%\n",
      "Finetune Total       98.38%  86.12%  96.48%\n",
      "Scratch ASED         98.95%  29.74%  88.21%\n",
      "Scratch EmoDB        20.18%  79.07%  29.31%\n",
      "\n",
      "================================================================================\n",
      "Cross-Lingual Evaluation Complete.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if RUN_CROSS_LINGUAL_EVALUATION:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### RUNNING PART 2: CROSS-LINGUAL EVALUATION (Max's Part) ###\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # --- 1. Load Datasets ---\n",
    "    print(\"--- Loading Cross-Lingual Datasets ---\")\n",
    "    df_ased = load_ased_data(cfg.ASED_DATASET_PATH)\n",
    "    df_emodb = load_emodb_data(cfg.EMODB_DATASET_PATH)\n",
    "    \n",
    "    df_ased['emotion'] = df_ased['emotion_str'].map(CANONICAL_EMOTION_TO_INT)\n",
    "    df_emodb['emotion'] = df_emodb['emotion_str'].map(CANONICAL_EMOTION_TO_INT)\n",
    "    df_ased.dropna(subset=['emotion'], inplace=True)\n",
    "    df_emodb.dropna(subset=['emotion'], inplace=True)\n",
    "    \n",
    "    df_total = pd.concat([df_ased, df_emodb], ignore_index=True)\n",
    "    print(f\"Loaded {len(df_ased)} ASED samples and {len(df_emodb)} EmoDB samples.\")\n",
    "\n",
    "    # --- 2. Setup Models ---\n",
    "    print(\"\\n--- Initializing Models ---\")\n",
    "    feature_params = {\n",
    "        'feature_type': 'mfcc',\n",
    "        'sr': cfg.SR_CROSS,\n",
    "        'n_mfcc': cfg.N_MFCC_CROSS,\n",
    "        'n_fft': cfg.N_FFT_MFCC_CROSS,\n",
    "        'hop_length': cfg.HOP_LENGTH_MFCC_CROSS,\n",
    "        'fixed_timesteps': cfg.FIXED_TIME_STEPS_CROSS\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        main_model = SpeechEmotionModel(num_classes=NUM_CLASSES, feature_dim=cfg.N_MFCC_CROSS).to(DEVICE)\n",
    "        main_model.load_state_dict(torch.load(cfg.CROSS_LINGUAL_PRETRAINED_MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"Successfully loaded pre-trained model.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Pre-trained model not found at '{cfg.CROSS_LINGUAL_PRETRAINED_MODEL_PATH}'.\")\n",
    "        print(\"Creating a dummy model with random weights for this part.\")\n",
    "        main_model = SpeechEmotionModel(num_classes=NUM_CLASSES, feature_dim=cfg.N_MFCC_CROSS).to(DEVICE)\n",
    "    \n",
    "    model_finetune_ased = copy.deepcopy(main_model)\n",
    "    model_finetune_emodb = copy.deepcopy(main_model)\n",
    "    model_finetune_total = copy.deepcopy(main_model)\n",
    "    model_scratch_ased = SpeechEmotionModel(num_classes=NUM_CLASSES, feature_dim=cfg.N_MFCC_CROSS).to(DEVICE)\n",
    "    model_scratch_emodb = SpeechEmotionModel(num_classes=NUM_CLASSES, feature_dim=cfg.N_MFCC_CROSS).to(DEVICE)\n",
    "\n",
    "    models_to_train = {\n",
    "        \"Finetune ASED\": model_finetune_ased,\n",
    "        \"Finetune EmoDB\": model_finetune_emodb,\n",
    "        \"Finetune Total\": model_finetune_total,\n",
    "        \"Scratch ASED\": model_scratch_ased,\n",
    "        \"Scratch EmoDB\": model_scratch_emodb\n",
    "    }\n",
    "\n",
    "    # --- 3. Split Data and Train Models ---\n",
    "    print(\"\\n--- Preparing Data and Starting Training ---\")\n",
    "    \n",
    "    ased_train, ased_test = train_test_split(df_ased, test_size=0.25, random_state=cfg.RANDOM_STATE, stratify=df_ased['emotion'])\n",
    "    emodb_train, emodb_test = train_test_split(df_emodb, test_size=0.25, random_state=cfg.RANDOM_STATE, stratify=df_emodb['emotion'])\n",
    "    total_train = pd.concat([ased_train, emodb_train], ignore_index=True)\n",
    "    total_test = pd.concat([ased_test, emodb_test], ignore_index=True)\n",
    "\n",
    "    datasets_for_training = {\n",
    "        \"Finetune ASED\": (ased_train, ased_test),\n",
    "        \"Finetune EmoDB\": (emodb_train, emodb_test),\n",
    "        \"Finetune Total\": (total_train, total_test),\n",
    "        \"Scratch ASED\": (ased_train, ased_test),\n",
    "        \"Scratch EmoDB\": (emodb_train, emodb_test)\n",
    "    }\n",
    "\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"\\n--- Training Model: {name} ---\")\n",
    "        train_df, test_df = datasets_for_training[name]\n",
    "        \n",
    "        train_dataset = OnTheFlyDataset(train_df, feature_params)\n",
    "        test_dataset = OnTheFlyDataset(test_df, feature_params)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE_CROSS, shuffle=True, collate_fn=pad_collate)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE_CROSS, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE_CROSS)\n",
    "        \n",
    "        pbar = tqdm(range(cfg.NUM_EPOCHS_CROSS), desc=f\"Training {name}\")\n",
    "        for epoch in pbar:\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                test_acc = evaluate_classification(model, test_loader, DEVICE)\n",
    "                pbar.set_description(f\"Training {name} (Loss: {train_loss:.4f}, Test Acc: {test_acc:.2f}%)\")\n",
    "\n",
    "    # --- 4. Evaluate All Models on All Test Sets ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### FINAL CROSS-LINGUAL EVALUATION RESULTS ###\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    eval_loaders = {\n",
    "        \"ASED\": DataLoader(OnTheFlyDataset(df_ased, feature_params), batch_size=cfg.BATCH_SIZE_CROSS, collate_fn=pad_collate),\n",
    "        \"EmoDB\": DataLoader(OnTheFlyDataset(df_emodb, feature_params), batch_size=cfg.BATCH_SIZE_CROSS, collate_fn=pad_collate),\n",
    "        \"Total\": DataLoader(OnTheFlyDataset(df_total, feature_params), batch_size=cfg.BATCH_SIZE_CROSS, collate_fn=pad_collate)\n",
    "    }\n",
    "\n",
    "    final_results = {}\n",
    "    all_models_for_eval = {\"Pre-trained English\": main_model, **models_to_train}\n",
    "\n",
    "    for model_name, model in all_models_for_eval.items():\n",
    "        result_row = {}\n",
    "        for loader_name, loader in eval_loaders.items():\n",
    "            accuracy = evaluate_classification(model, loader, DEVICE)\n",
    "            result_row[loader_name] = f\"{accuracy:.2f}%\"\n",
    "        final_results[model_name] = result_row\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient='index')\n",
    "    print(results_df)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\nCross-Lingual Evaluation Complete.\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "depression-code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### RUNNING PART 3: DEPRESSION DETECTION (PERFECT REPLICATION) ###\n",
      "================================================================================\n",
      "\n",
      "Loading saved models...\n",
      "Loaded 181 DAIC-WOZ samples\n",
      "Extracting emotion features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3224e784de1b461e9bacd224176c682c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created feature DataFrame with 181 samples\n",
      "\n",
      "Augmenting depression cases...\n",
      "Augmented to 337 samples\n",
      "Depression cases: 190 (56.4%)\n",
      "\n",
      "Train: 215, Val: 54, Test: 68\n",
      "\n",
      "Training for 150 epochs...\n",
      "Epoch [1/150], Batch [3/7], Loss: 379.2982\n",
      "Epoch [1/150], Batch [6/7], Loss: 783.3206\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.22\n",
      "RMSE: 5.42\n",
      "R²: 0.035\n",
      "Predictions >= 10: 18 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.685\n",
      "F1-Score: 0.653\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 21, FP: 2\n",
      "FN: 15, TP: 16\n",
      "New best model! F1: 0.653, MAE: 4.22\n",
      "Epoch [2/150], Batch [3/7], Loss: 572.6677\n",
      "Epoch [2/150], Batch [6/7], Loss: 297.5542\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.02\n",
      "RMSE: 5.26\n",
      "R²: 0.091\n",
      "Predictions >= 10: 28 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.833\n",
      "F1-Score: 0.847\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 20, FP: 3\n",
      "FN: 6, TP: 25\n",
      "New best model! F1: 0.847, MAE: 4.02\n",
      "Epoch [3/150], Batch [3/7], Loss: 455.1013\n",
      "Epoch [3/150], Batch [6/7], Loss: 337.8681\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 3.87\n",
      "RMSE: 5.07\n",
      "R²: 0.157\n",
      "Predictions >= 10: 28 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.833\n",
      "F1-Score: 0.847\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 20, FP: 3\n",
      "FN: 6, TP: 25\n",
      "New best model! F1: 0.847, MAE: 3.87\n",
      "Epoch [4/150], Batch [3/7], Loss: 264.3672\n",
      "Epoch [4/150], Batch [6/7], Loss: 478.8442\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 3.83\n",
      "RMSE: 4.97\n",
      "R²: 0.190\n",
      "Predictions >= 10: 31 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.778\n",
      "F1-Score: 0.806\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 17, FP: 6\n",
      "FN: 6, TP: 25\n",
      "Epoch [5/150], Batch [3/7], Loss: 411.3111\n",
      "Epoch [5/150], Batch [6/7], Loss: 326.3084\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 3.86\n",
      "RMSE: 4.95\n",
      "R²: 0.196\n",
      "Predictions >= 10: 31 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.778\n",
      "F1-Score: 0.806\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 17, FP: 6\n",
      "FN: 6, TP: 25\n",
      "Epoch [6/150], Batch [3/7], Loss: 405.3008\n",
      "Epoch [6/150], Batch [6/7], Loss: 231.1477\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 3.91\n",
      "RMSE: 4.99\n",
      "R²: 0.182\n",
      "Predictions >= 10: 35 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.704\n",
      "F1-Score: 0.758\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 13, FP: 10\n",
      "FN: 6, TP: 25\n",
      "Epoch [7/150], Batch [3/7], Loss: 254.0023\n",
      "Epoch [7/150], Batch [6/7], Loss: 209.2407\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 3.98\n",
      "RMSE: 5.07\n",
      "R²: 0.157\n",
      "Predictions >= 10: 49 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.667\n",
      "F1-Score: 0.775\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 5, FP: 18\n",
      "FN: 0, TP: 31\n",
      "Epoch [8/150], Batch [3/7], Loss: 313.0509\n",
      "Epoch [8/150], Batch [6/7], Loss: 338.3236\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.07\n",
      "RMSE: 5.15\n",
      "R²: 0.130\n",
      "Predictions >= 10: 53 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.593\n",
      "F1-Score: 0.738\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 1, FP: 22\n",
      "FN: 0, TP: 31\n",
      "Epoch [9/150], Batch [3/7], Loss: 286.7542\n",
      "Epoch [9/150], Batch [6/7], Loss: 367.9654\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.19\n",
      "RMSE: 5.27\n",
      "R²: 0.089\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [10/150], Batch [3/7], Loss: 287.0237\n",
      "Epoch [10/150], Batch [6/7], Loss: 217.9943\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.33\n",
      "RMSE: 5.43\n",
      "R²: 0.034\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [11/150], Batch [3/7], Loss: 304.9600\n",
      "Epoch [11/150], Batch [6/7], Loss: 208.9093\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.40\n",
      "RMSE: 5.50\n",
      "R²: 0.007\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [12/150], Batch [3/7], Loss: 182.3559\n",
      "Epoch [12/150], Batch [6/7], Loss: 443.6064\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.47\n",
      "RMSE: 5.57\n",
      "R²: -0.020\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [13/150], Batch [3/7], Loss: 161.3035\n",
      "Epoch [13/150], Batch [6/7], Loss: 197.9795\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.52\n",
      "RMSE: 5.63\n",
      "R²: -0.042\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [14/150], Batch [3/7], Loss: 195.7625\n",
      "Epoch [14/150], Batch [6/7], Loss: 177.4521\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.56\n",
      "RMSE: 5.70\n",
      "R²: -0.066\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [15/150], Batch [3/7], Loss: 201.5854\n",
      "Epoch [15/150], Batch [6/7], Loss: 236.9815\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.62\n",
      "RMSE: 5.77\n",
      "R²: -0.092\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [16/150], Batch [3/7], Loss: 197.7513\n",
      "Epoch [16/150], Batch [6/7], Loss: 230.9299\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.66\n",
      "RMSE: 5.83\n",
      "R²: -0.115\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [17/150], Batch [3/7], Loss: 249.1946\n",
      "Epoch [17/150], Batch [6/7], Loss: 200.2013\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.69\n",
      "RMSE: 5.87\n",
      "R²: -0.130\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [18/150], Batch [3/7], Loss: 204.1528\n",
      "Epoch [18/150], Batch [6/7], Loss: 115.6061\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.72\n",
      "RMSE: 5.90\n",
      "R²: -0.141\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [19/150], Batch [3/7], Loss: 303.4753\n",
      "Epoch [19/150], Batch [6/7], Loss: 226.8952\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.74\n",
      "RMSE: 5.94\n",
      "R²: -0.157\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [20/150], Batch [3/7], Loss: 285.4086\n",
      "Epoch [20/150], Batch [6/7], Loss: 279.2816\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.79\n",
      "RMSE: 5.99\n",
      "R²: -0.178\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [20/150]\n",
      "Train Loss: 226.1303, Val MAE: 4.79, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [21/150], Batch [3/7], Loss: 284.4974\n",
      "Epoch [21/150], Batch [6/7], Loss: 137.5624\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.82\n",
      "RMSE: 6.03\n",
      "R²: -0.195\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [22/150], Batch [3/7], Loss: 197.1280\n",
      "Epoch [22/150], Batch [6/7], Loss: 383.6893\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.83\n",
      "RMSE: 6.07\n",
      "R²: -0.209\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [23/150], Batch [3/7], Loss: 190.9255\n",
      "Epoch [23/150], Batch [6/7], Loss: 224.3872\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.86\n",
      "RMSE: 6.11\n",
      "R²: -0.225\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [24/150], Batch [3/7], Loss: 320.5790\n",
      "Epoch [24/150], Batch [6/7], Loss: 234.5273\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.89\n",
      "RMSE: 6.14\n",
      "R²: -0.236\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [25/150], Batch [3/7], Loss: 136.2239\n",
      "Epoch [25/150], Batch [6/7], Loss: 205.2834\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.92\n",
      "RMSE: 6.18\n",
      "R²: -0.252\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [26/150], Batch [3/7], Loss: 247.8551\n",
      "Epoch [26/150], Batch [6/7], Loss: 315.2903\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.95\n",
      "RMSE: 6.21\n",
      "R²: -0.264\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [27/150], Batch [3/7], Loss: 166.6855\n",
      "Epoch [27/150], Batch [6/7], Loss: 154.3619\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.95\n",
      "RMSE: 6.22\n",
      "R²: -0.270\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [28/150], Batch [3/7], Loss: 256.0311\n",
      "Epoch [28/150], Batch [6/7], Loss: 247.5134\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.98\n",
      "RMSE: 6.27\n",
      "R²: -0.288\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [29/150], Batch [3/7], Loss: 181.1952\n",
      "Epoch [29/150], Batch [6/7], Loss: 260.1247\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.98\n",
      "RMSE: 6.26\n",
      "R²: -0.286\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [30/150], Batch [3/7], Loss: 233.4259\n",
      "Epoch [30/150], Batch [6/7], Loss: 171.4588\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.99\n",
      "RMSE: 6.27\n",
      "R²: -0.291\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [31/150], Batch [3/7], Loss: 242.9937\n",
      "Epoch [31/150], Batch [6/7], Loss: 134.1509\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.29\n",
      "R²: -0.299\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [32/150], Batch [3/7], Loss: 220.2693\n",
      "Epoch [32/150], Batch [6/7], Loss: 219.1188\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.30\n",
      "R²: -0.303\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [33/150], Batch [3/7], Loss: 320.0296\n",
      "Epoch [33/150], Batch [6/7], Loss: 255.0176\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.30\n",
      "R²: -0.302\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [34/150], Batch [3/7], Loss: 110.4075\n",
      "Epoch [34/150], Batch [6/7], Loss: 112.0410\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.96\n",
      "RMSE: 6.25\n",
      "R²: -0.284\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [35/150], Batch [3/7], Loss: 119.0792\n",
      "Epoch [35/150], Batch [6/7], Loss: 147.8491\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.98\n",
      "RMSE: 6.27\n",
      "R²: -0.289\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [36/150], Batch [3/7], Loss: 211.4669\n",
      "Epoch [36/150], Batch [6/7], Loss: 248.0043\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.30\n",
      "R²: -0.301\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [37/150], Batch [3/7], Loss: 169.1115\n",
      "Epoch [37/150], Batch [6/7], Loss: 203.2608\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.99\n",
      "RMSE: 6.29\n",
      "R²: -0.297\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [38/150], Batch [3/7], Loss: 271.0775\n",
      "Epoch [38/150], Batch [6/7], Loss: 255.1543\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.98\n",
      "RMSE: 6.27\n",
      "R²: -0.291\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [39/150], Batch [3/7], Loss: 147.9198\n",
      "Epoch [39/150], Batch [6/7], Loss: 99.5692\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.29\n",
      "R²: -0.301\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [40/150], Batch [3/7], Loss: 180.9109\n",
      "Epoch [40/150], Batch [6/7], Loss: 164.8232\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.30\n",
      "R²: -0.302\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [40/150]\n",
      "Train Loss: 201.2715, Val MAE: 5.00, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [41/150], Batch [3/7], Loss: 267.9427\n",
      "Epoch [41/150], Batch [6/7], Loss: 178.3363\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [42/150], Batch [3/7], Loss: 192.8591\n",
      "Epoch [42/150], Batch [6/7], Loss: 120.9693\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.306\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [43/150], Batch [3/7], Loss: 147.4153\n",
      "Epoch [43/150], Batch [6/7], Loss: 157.8992\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [44/150], Batch [3/7], Loss: 246.7279\n",
      "Epoch [44/150], Batch [6/7], Loss: 208.1953\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [45/150], Batch [3/7], Loss: 165.9249\n",
      "Epoch [45/150], Batch [6/7], Loss: 159.6617\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.306\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [46/150], Batch [3/7], Loss: 220.3533\n",
      "Epoch [46/150], Batch [6/7], Loss: 161.8604\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [47/150], Batch [3/7], Loss: 89.7904\n",
      "Epoch [47/150], Batch [6/7], Loss: 125.1025\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.318\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [48/150], Batch [3/7], Loss: 181.3183\n",
      "Epoch [48/150], Batch [6/7], Loss: 193.5058\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [49/150], Batch [3/7], Loss: 206.6374\n",
      "Epoch [49/150], Batch [6/7], Loss: 179.3850\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.07\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [50/150], Batch [3/7], Loss: 167.4345\n",
      "Epoch [50/150], Batch [6/7], Loss: 248.1489\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [51/150], Batch [3/7], Loss: 193.7048\n",
      "Epoch [51/150], Batch [6/7], Loss: 175.3220\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.329\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [52/150], Batch [3/7], Loss: 232.0150\n",
      "Epoch [52/150], Batch [6/7], Loss: 263.3629\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.324\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [53/150], Batch [3/7], Loss: 214.4641\n",
      "Epoch [53/150], Batch [6/7], Loss: 116.5549\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [54/150], Batch [3/7], Loss: 195.4660\n",
      "Epoch [54/150], Batch [6/7], Loss: 227.9886\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.36\n",
      "R²: -0.326\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [55/150], Batch [3/7], Loss: 188.8858\n",
      "Epoch [55/150], Batch [6/7], Loss: 207.5166\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.35\n",
      "R²: -0.323\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [56/150], Batch [3/7], Loss: 193.0155\n",
      "Epoch [56/150], Batch [6/7], Loss: 256.1555\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.318\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [57/150], Batch [3/7], Loss: 216.7010\n",
      "Epoch [57/150], Batch [6/7], Loss: 233.3326\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.326\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [58/150], Batch [3/7], Loss: 145.1492\n",
      "Epoch [58/150], Batch [6/7], Loss: 141.5667\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.325\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [59/150], Batch [3/7], Loss: 102.2708\n",
      "Epoch [59/150], Batch [6/7], Loss: 167.0354\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [60/150], Batch [3/7], Loss: 187.6167\n",
      "Epoch [60/150], Batch [6/7], Loss: 180.4228\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.317\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [60/150]\n",
      "Train Loss: 209.0635, Val MAE: 5.03, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [61/150], Batch [3/7], Loss: 259.7093\n",
      "Epoch [61/150], Batch [6/7], Loss: 241.5380\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.34\n",
      "R²: -0.318\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [62/150], Batch [3/7], Loss: 216.4033\n",
      "Epoch [62/150], Batch [6/7], Loss: 261.7661\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.324\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [63/150], Batch [3/7], Loss: 117.0213\n",
      "Epoch [63/150], Batch [6/7], Loss: 143.5548\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.35\n",
      "R²: -0.324\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [64/150], Batch [3/7], Loss: 163.1438\n",
      "Epoch [64/150], Batch [6/7], Loss: 199.4349\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.08\n",
      "RMSE: 6.37\n",
      "R²: -0.332\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [65/150], Batch [3/7], Loss: 341.5861\n",
      "Epoch [65/150], Batch [6/7], Loss: 109.1988\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.35\n",
      "R²: -0.325\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [66/150], Batch [3/7], Loss: 192.5095\n",
      "Epoch [66/150], Batch [6/7], Loss: 149.4092\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [67/150], Batch [3/7], Loss: 165.6542\n",
      "Epoch [67/150], Batch [6/7], Loss: 139.2463\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [68/150], Batch [3/7], Loss: 170.4216\n",
      "Epoch [68/150], Batch [6/7], Loss: 98.0220\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.32\n",
      "R²: -0.309\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [69/150], Batch [3/7], Loss: 204.8715\n",
      "Epoch [69/150], Batch [6/7], Loss: 116.6167\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.321\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [70/150], Batch [3/7], Loss: 149.2248\n",
      "Epoch [70/150], Batch [6/7], Loss: 206.4707\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.08\n",
      "RMSE: 6.38\n",
      "R²: -0.335\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [71/150], Batch [3/7], Loss: 115.4259\n",
      "Epoch [71/150], Batch [6/7], Loss: 158.1353\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.321\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [72/150], Batch [3/7], Loss: 99.7562\n",
      "Epoch [72/150], Batch [6/7], Loss: 319.9049\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.29\n",
      "R²: -0.297\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [73/150], Batch [3/7], Loss: 129.9515\n",
      "Epoch [73/150], Batch [6/7], Loss: 239.7529\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.29\n",
      "R²: -0.300\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [74/150], Batch [3/7], Loss: 152.8852\n",
      "Epoch [74/150], Batch [6/7], Loss: 141.1553\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.31\n",
      "R²: -0.308\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [75/150], Batch [3/7], Loss: 216.7150\n",
      "Epoch [75/150], Batch [6/7], Loss: 187.3526\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [76/150], Batch [3/7], Loss: 94.2457\n",
      "Epoch [76/150], Batch [6/7], Loss: 150.1682\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.316\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [77/150], Batch [3/7], Loss: 217.1046\n",
      "Epoch [77/150], Batch [6/7], Loss: 194.3958\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.08\n",
      "RMSE: 6.38\n",
      "R²: -0.337\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [78/150], Batch [3/7], Loss: 255.9559\n",
      "Epoch [78/150], Batch [6/7], Loss: 274.9135\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.325\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [79/150], Batch [3/7], Loss: 88.9830\n",
      "Epoch [79/150], Batch [6/7], Loss: 148.2108\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.36\n",
      "R²: -0.329\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [80/150], Batch [3/7], Loss: 128.3898\n",
      "Epoch [80/150], Batch [6/7], Loss: 144.2936\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.329\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [80/150]\n",
      "Train Loss: 177.2415, Val MAE: 5.06, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [81/150], Batch [3/7], Loss: 281.8566\n",
      "Epoch [81/150], Batch [6/7], Loss: 309.1069\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [82/150], Batch [3/7], Loss: 185.9292\n",
      "Epoch [82/150], Batch [6/7], Loss: 161.7938\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.09\n",
      "RMSE: 6.40\n",
      "R²: -0.343\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [83/150], Batch [3/7], Loss: 176.6986\n",
      "Epoch [83/150], Batch [6/7], Loss: 169.7242\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [84/150], Batch [3/7], Loss: 204.6514\n",
      "Epoch [84/150], Batch [6/7], Loss: 271.0532\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.320\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [85/150], Batch [3/7], Loss: 234.6254\n",
      "Epoch [85/150], Batch [6/7], Loss: 241.0835\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.35\n",
      "R²: -0.325\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [86/150], Batch [3/7], Loss: 162.1476\n",
      "Epoch [86/150], Batch [6/7], Loss: 255.0807\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.316\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [87/150], Batch [3/7], Loss: 229.2974\n",
      "Epoch [87/150], Batch [6/7], Loss: 246.0298\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.313\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [88/150], Batch [3/7], Loss: 195.3082\n",
      "Epoch [88/150], Batch [6/7], Loss: 118.4730\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.31\n",
      "R²: -0.309\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [89/150], Batch [3/7], Loss: 189.4526\n",
      "Epoch [89/150], Batch [6/7], Loss: 306.0949\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.07\n",
      "RMSE: 6.37\n",
      "R²: -0.332\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [90/150], Batch [3/7], Loss: 273.3441\n",
      "Epoch [90/150], Batch [6/7], Loss: 132.0887\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [91/150], Batch [3/7], Loss: 205.4147\n",
      "Epoch [91/150], Batch [6/7], Loss: 180.3945\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [92/150], Batch [3/7], Loss: 142.2947\n",
      "Epoch [92/150], Batch [6/7], Loss: 260.1780\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.320\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [93/150], Batch [3/7], Loss: 284.2027\n",
      "Epoch [93/150], Batch [6/7], Loss: 116.1246\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [94/150], Batch [3/7], Loss: 110.1422\n",
      "Epoch [94/150], Batch [6/7], Loss: 157.4184\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.307\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [95/150], Batch [3/7], Loss: 185.3832\n",
      "Epoch [95/150], Batch [6/7], Loss: 211.0301\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.33\n",
      "R²: -0.314\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [96/150], Batch [3/7], Loss: 217.4544\n",
      "Epoch [96/150], Batch [6/7], Loss: 144.0505\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.313\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [97/150], Batch [3/7], Loss: 137.7440\n",
      "Epoch [97/150], Batch [6/7], Loss: 260.7044\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.329\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [98/150], Batch [3/7], Loss: 189.3493\n",
      "Epoch [98/150], Batch [6/7], Loss: 172.0317\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.325\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [99/150], Batch [3/7], Loss: 126.4626\n",
      "Epoch [99/150], Batch [6/7], Loss: 224.2209\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.320\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [100/150], Batch [3/7], Loss: 119.3482\n",
      "Epoch [100/150], Batch [6/7], Loss: 301.8435\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [100/150]\n",
      "Train Loss: 195.7813, Val MAE: 5.03, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [101/150], Batch [3/7], Loss: 172.2931\n",
      "Epoch [101/150], Batch [6/7], Loss: 351.3721\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.318\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [102/150], Batch [3/7], Loss: 88.2106\n",
      "Epoch [102/150], Batch [6/7], Loss: 140.8610\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.305\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [103/150], Batch [3/7], Loss: 145.5331\n",
      "Epoch [103/150], Batch [6/7], Loss: 191.3756\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.324\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [104/150], Batch [3/7], Loss: 138.6311\n",
      "Epoch [104/150], Batch [6/7], Loss: 197.8951\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.314\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [105/150], Batch [3/7], Loss: 218.9830\n",
      "Epoch [105/150], Batch [6/7], Loss: 164.3470\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.30\n",
      "R²: -0.304\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [106/150], Batch [3/7], Loss: 176.6374\n",
      "Epoch [106/150], Batch [6/7], Loss: 228.7086\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [107/150], Batch [3/7], Loss: 132.9531\n",
      "Epoch [107/150], Batch [6/7], Loss: 269.6653\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.323\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [108/150], Batch [3/7], Loss: 179.5580\n",
      "Epoch [108/150], Batch [6/7], Loss: 135.9986\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.320\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [109/150], Batch [3/7], Loss: 163.8838\n",
      "Epoch [109/150], Batch [6/7], Loss: 123.9022\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.32\n",
      "R²: -0.310\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [110/150], Batch [3/7], Loss: 140.6364\n",
      "Epoch [110/150], Batch [6/7], Loss: 227.8687\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.326\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [111/150], Batch [3/7], Loss: 263.4182\n",
      "Epoch [111/150], Batch [6/7], Loss: 155.3550\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [112/150], Batch [3/7], Loss: 151.5296\n",
      "Epoch [112/150], Batch [6/7], Loss: 259.9562\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.07\n",
      "RMSE: 6.37\n",
      "R²: -0.331\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [113/150], Batch [3/7], Loss: 234.8939\n",
      "Epoch [113/150], Batch [6/7], Loss: 177.4895\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.322\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [114/150], Batch [3/7], Loss: 159.7188\n",
      "Epoch [114/150], Batch [6/7], Loss: 143.9417\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.326\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [115/150], Batch [3/7], Loss: 184.6793\n",
      "Epoch [115/150], Batch [6/7], Loss: 176.4254\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [116/150], Batch [3/7], Loss: 151.6131\n",
      "Epoch [116/150], Batch [6/7], Loss: 223.0254\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.99\n",
      "RMSE: 6.30\n",
      "R²: -0.303\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [117/150], Batch [3/7], Loss: 214.5896\n",
      "Epoch [117/150], Batch [6/7], Loss: 262.3073\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.308\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [118/150], Batch [3/7], Loss: 198.5756\n",
      "Epoch [118/150], Batch [6/7], Loss: 272.1072\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.98\n",
      "RMSE: 6.28\n",
      "R²: -0.294\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [119/150], Batch [3/7], Loss: 285.0788\n",
      "Epoch [119/150], Batch [6/7], Loss: 217.0266\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.314\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [120/150], Batch [3/7], Loss: 240.9682\n",
      "Epoch [120/150], Batch [6/7], Loss: 227.7412\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [120/150]\n",
      "Train Loss: 198.9690, Val MAE: 5.04, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [121/150], Batch [3/7], Loss: 253.9452\n",
      "Epoch [121/150], Batch [6/7], Loss: 163.3777\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [122/150], Batch [3/7], Loss: 200.4957\n",
      "Epoch [122/150], Batch [6/7], Loss: 159.2126\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.318\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [123/150], Batch [3/7], Loss: 134.3536\n",
      "Epoch [123/150], Batch [6/7], Loss: 232.0154\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.01\n",
      "RMSE: 6.31\n",
      "R²: -0.308\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [124/150], Batch [3/7], Loss: 228.0673\n",
      "Epoch [124/150], Batch [6/7], Loss: 192.6089\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.314\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [125/150], Batch [3/7], Loss: 214.2784\n",
      "Epoch [125/150], Batch [6/7], Loss: 196.6549\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.32\n",
      "R²: -0.313\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [126/150], Batch [3/7], Loss: 347.7103\n",
      "Epoch [126/150], Batch [6/7], Loss: 157.3535\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [127/150], Batch [3/7], Loss: 216.1257\n",
      "Epoch [127/150], Batch [6/7], Loss: 253.2626\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.313\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [128/150], Batch [3/7], Loss: 177.2558\n",
      "Epoch [128/150], Batch [6/7], Loss: 311.8749\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.313\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [129/150], Batch [3/7], Loss: 222.1779\n",
      "Epoch [129/150], Batch [6/7], Loss: 205.1471\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.07\n",
      "RMSE: 6.37\n",
      "R²: -0.333\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [130/150], Batch [3/7], Loss: 201.7523\n",
      "Epoch [130/150], Batch [6/7], Loss: 182.9374\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.35\n",
      "R²: -0.324\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [131/150], Batch [3/7], Loss: 107.0163\n",
      "Epoch [131/150], Batch [6/7], Loss: 235.7495\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.31\n",
      "R²: -0.306\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [132/150], Batch [3/7], Loss: 146.1248\n",
      "Epoch [132/150], Batch [6/7], Loss: 215.3491\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [133/150], Batch [3/7], Loss: 92.1373\n",
      "Epoch [133/150], Batch [6/7], Loss: 210.2638\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.32\n",
      "R²: -0.311\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [134/150], Batch [3/7], Loss: 213.1425\n",
      "Epoch [134/150], Batch [6/7], Loss: 175.8026\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [135/150], Batch [3/7], Loss: 344.8839\n",
      "Epoch [135/150], Batch [6/7], Loss: 212.9925\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [136/150], Batch [3/7], Loss: 256.5182\n",
      "Epoch [136/150], Batch [6/7], Loss: 203.7798\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.328\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [137/150], Batch [3/7], Loss: 182.6561\n",
      "Epoch [137/150], Batch [6/7], Loss: 194.9395\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.327\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [138/150], Batch [3/7], Loss: 209.8552\n",
      "Epoch [138/150], Batch [6/7], Loss: 175.6978\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.07\n",
      "RMSE: 6.37\n",
      "R²: -0.332\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [139/150], Batch [3/7], Loss: 236.6011\n",
      "Epoch [139/150], Batch [6/7], Loss: 154.4427\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.06\n",
      "RMSE: 6.36\n",
      "R²: -0.328\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [140/150], Batch [3/7], Loss: 121.9610\n",
      "Epoch [140/150], Batch [6/7], Loss: 151.1525\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.315\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "Epoch [140/150]\n",
      "Train Loss: 191.3943, Val MAE: 5.03, Val F1: 0.729\n",
      "Predicting 54/54 as depressed\n",
      "Epoch [141/150], Batch [3/7], Loss: 254.5272\n",
      "Epoch [141/150], Batch [6/7], Loss: 166.7116\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [142/150], Batch [3/7], Loss: 246.1457\n",
      "Epoch [142/150], Batch [6/7], Loss: 238.3226\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [143/150], Batch [3/7], Loss: 331.7256\n",
      "Epoch [143/150], Batch [6/7], Loss: 141.7483\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.31\n",
      "R²: -0.308\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [144/150], Batch [3/7], Loss: 272.7010\n",
      "Epoch [144/150], Batch [6/7], Loss: 119.3842\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.02\n",
      "RMSE: 6.31\n",
      "R²: -0.308\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [145/150], Batch [3/7], Loss: 184.4997\n",
      "Epoch [145/150], Batch [6/7], Loss: 183.0914\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.29\n",
      "R²: -0.298\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [146/150], Batch [3/7], Loss: 286.7646\n",
      "Epoch [146/150], Batch [6/7], Loss: 204.9464\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.03\n",
      "RMSE: 6.33\n",
      "R²: -0.314\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [147/150], Batch [3/7], Loss: 122.9728\n",
      "Epoch [147/150], Batch [6/7], Loss: 236.0309\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.34\n",
      "R²: -0.319\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [148/150], Batch [3/7], Loss: 303.4121\n",
      "Epoch [148/150], Batch [6/7], Loss: 265.8873\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.05\n",
      "RMSE: 6.34\n",
      "R²: -0.321\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [149/150], Batch [3/7], Loss: 238.8822\n",
      "Epoch [149/150], Batch [6/7], Loss: 248.8521\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.00\n",
      "RMSE: 6.30\n",
      "R²: -0.303\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "Epoch [150/150], Batch [3/7], Loss: 199.5853\n",
      "Epoch [150/150], Batch [6/7], Loss: 164.4007\n",
      "Warning: Model predicts all samples as same class\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 5.04\n",
      "RMSE: 6.33\n",
      "R²: -0.316\n",
      "Predictions >= 10: 54 out of 54\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.574\n",
      "F1-Score: 0.000\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 23\n",
      "FN: 0, TP: 31\n",
      "\n",
      "==================================================\n",
      "FINAL TEST EVALUATION\n",
      "==================================================\n",
      "\n",
      "=== Regression Evaluation ===\n",
      "MAE: 4.09\n",
      "RMSE: 5.35\n",
      "R²: 0.093\n",
      "Predictions >= 10: 31 out of 68\n",
      "\n",
      "Binary Classification (PHQ-8 >= 10):\n",
      "Accuracy: 0.779\n",
      "F1-Score: 0.783\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 26, FP: 4\n",
      "FN: 11, TP: 27\n",
      "\n",
      "================================================================================\n",
      "Depression Detection (Perfect Replication) Complete.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if RUN_DEPRESSION_DETECTION:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### RUNNING PART 3: DEPRESSION DETECTION (PERFECT REPLICATION) ###\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    # not working properly with rest of the code so added all the functions here.\n",
    "    def local_extract_mfcc(file_path, sr, duration, n_mfcc, n_fft, hop_length):\n",
    "        try:\n",
    "            audio, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "            target_len = int(sr * duration)\n",
    "            if len(audio) < target_len:\n",
    "                audio = np.pad(audio, (0, target_len - len(audio)), 'constant')\n",
    "            else:\n",
    "                audio = audio[:target_len]\n",
    "            mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "            return mfcc_features.T\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing MFCC for {file_path}: {e}\")\n",
    "            num_time_steps = int(np.ceil(int(sr * duration) / hop_length))\n",
    "            return np.zeros((num_time_steps, n_mfcc))\n",
    "\n",
    "    def local_load_daic_woz_data(dataset_path):\n",
    "        data = []\n",
    "        try:\n",
    "            depression_info_train = os.path.join(dataset_path, \"train_split_Depression_AVEC2017.csv\")\n",
    "            depression_info_test = os.path.join(dataset_path, \"full_test_split.csv\")\n",
    "            df_train = pd.read_csv(depression_info_train)\n",
    "            df_test = pd.read_csv(depression_info_test)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"DAIC-WOZ CSVs not found in {dataset_path}. Cannot proceed.\")\n",
    "            return pd.DataFrame(data)\n",
    "        \n",
    "        if os.path.isdir(dataset_path):\n",
    "            audio = False\n",
    "            score = False\n",
    "            for file in os.listdir(dataset_path):\n",
    "                if file.endswith('.wav'):\n",
    "                    audio = True\n",
    "                    file_path = os.path.join(dataset_path, file)\n",
    "                    number = int(file.split('_')[0])\n",
    "                    if not df_train[df_train['Participant_ID'] == number].empty:\n",
    "                        score = True\n",
    "                        row = df_train[df_train['Participant_ID'] == number]\n",
    "                        phq8 = row['PHQ8_Score'].iloc[0]\n",
    "                    elif not df_test[df_test['Participant_ID'] == number].empty:\n",
    "                        score = True\n",
    "                        row = df_test[df_test['Participant_ID'] == number]\n",
    "                        phq8 = row['PHQ_Score'].iloc[0]\n",
    "                    if audio and score:\n",
    "                        data.append({'path': file_path, 'emotion': phq8})\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    class LocalSpeechAudioDataset(Dataset):\n",
    "        def __init__(self, dataframe, feature_type='mfcc', n_features=40):\n",
    "            self.dataframe = dataframe\n",
    "            self.feature_type = feature_type\n",
    "            self.n_features = n_features\n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "        def __getitem__(self, idx):\n",
    "            file_path = self.dataframe.iloc[idx]['path']\n",
    "            emotion = self.dataframe.iloc[idx]['emotion']\n",
    "            features = local_extract_mfcc(file_path, sr=cfg.SR_DEP, duration=cfg.DURATION_DEP, n_mfcc=self.n_features, n_fft=cfg.N_FFT_DEP, hop_length=cfg.HOP_LENGTH_DEP)\n",
    "            return features, emotion\n",
    "\n",
    "    def local_pad_collate(batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        max_len = max(x_item.shape[0] for x_item in xx)\n",
    "        padded_xx = []\n",
    "        for x_item in xx:\n",
    "            num_features = x_item.shape[1]\n",
    "            if x_item.shape[0] < max_len:\n",
    "                padding = np.zeros((max_len - x_item.shape[0], num_features))\n",
    "                padded_x_item = np.concatenate((x_item, padding), axis=0)\n",
    "            else:\n",
    "                padded_x_item = x_item[:max_len, :]\n",
    "            padded_xx.append(padded_x_item)\n",
    "        xx_pad = torch.tensor(np.array(padded_xx), dtype=torch.float32)\n",
    "        yy = torch.tensor(yy, dtype=torch.long)\n",
    "        return xx_pad, yy\n",
    "\n",
    "    class LocalAttention(nn.Module):\n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.attention_net = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
    "        def forward(self, lstm_output):\n",
    "            energies = self.attention_net(lstm_output).squeeze(2)\n",
    "            weights = F.softmax(energies, dim=1)\n",
    "            return lstm_output * weights.unsqueeze(2), weights\n",
    "\n",
    "    class LocalSpeechEmotionModel(nn.Module):\n",
    "        def __init__(self, num_classes, input_feature_dim):\n",
    "            super().__init__()\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "                nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "                nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "                nn.Conv2d(64, 64, 3, padding='same'), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "            lstm_input_size = 64 * (input_feature_dim // 16)\n",
    "            if lstm_input_size == 0: lstm_input_size = 64\n",
    "            self.lstm1 = nn.LSTM(lstm_input_size, 32, batch_first=True)\n",
    "            self.attention = LocalAttention(32)\n",
    "            self.lstm2 = nn.LSTM(32, 32, batch_first=True)\n",
    "            self.fc = nn.Linear(32, num_classes)\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)\n",
    "            x = self.conv_blocks(x)\n",
    "            b, c, t, f = x.shape\n",
    "            x = x.permute(0, 2, 1, 3).reshape(b, t, c * f)\n",
    "            self.lstm1.flatten_parameters()\n",
    "            x, _ = self.lstm1(x)\n",
    "            x, _ = self.attention(x)\n",
    "            self.lstm2.flatten_parameters()\n",
    "            _, (h_n, _) = self.lstm2(x)\n",
    "            return self.fc(h_n.squeeze(0))\n",
    "\n",
    "    class LocalDepressionDataset(Dataset):\n",
    "        def __init__(self, dataframe):\n",
    "            self.X = torch.tensor(dataframe['values'].tolist(), dtype=torch.float32)\n",
    "            self.y = torch.tensor(dataframe['phq8'].values, dtype=torch.float32)\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    class LocalDepressionModel(nn.Module):\n",
    "        def __init__(self, n_inputs=8, hidden_dims=[64, 32], dropout_rate=0.3):\n",
    "            super(LocalDepressionModel, self).__init__()\n",
    "            layers = []\n",
    "            prev_dim = n_inputs\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.extend([nn.Linear(prev_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(), nn.Dropout(dropout_rate)])\n",
    "                prev_dim = hidden_dim\n",
    "            layers.append(nn.Linear(prev_dim, 1))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        def forward(self, x):\n",
    "            return self.model(x).squeeze()\n",
    "            \n",
    "    class LocalWeightedMSELoss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(LocalWeightedMSELoss, self).__init__()\n",
    "        def forward(self, pred, target):\n",
    "            weights = torch.where(target >= 10, 10.0, 1.0)\n",
    "            error = pred - target\n",
    "            under_prediction_penalty = torch.where((target >= 10) & (error < 0), 3.0, 1.0)\n",
    "            mse = (pred - target) ** 2\n",
    "            weighted_mse = weights * under_prediction_penalty * mse\n",
    "            pred_std = pred.std()\n",
    "            if pred_std < 1.0:\n",
    "                diversity_penalty = (1.0 - pred_std) * 10.0\n",
    "                return torch.mean(weighted_mse) + diversity_penalty\n",
    "            return torch.mean(weighted_mse)\n",
    "            \n",
    "    def local_train_model_epoch(model, train_loader, criterion, optimizer, device, current_epoch, total_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            if (i + 1) % max(1, len(train_loader) // 2) == 0 and i > 0:\n",
    "                print(f'Epoch [{current_epoch+1}/{total_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        return running_loss / len(train_loader.dataset)\n",
    "\n",
    "    def local_evaluate_model_regression(model, test_loader, device):\n",
    "        model.eval()\n",
    "        predictions, true_values = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions.extend(outputs.cpu().numpy()); true_values.extend(labels.cpu().numpy())\n",
    "        predictions, true_values = np.array(predictions), np.array(true_values)\n",
    "        mae = mean_absolute_error(true_values, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "        if np.var(predictions) == 0: r2 = -1.0; print(\"Warning: Model is predicting constant values\")\n",
    "        else: r2 = r2_score(true_values, predictions)\n",
    "        binary_true = (true_values >= 10).astype(int); binary_pred = (predictions >= 10).astype(int)\n",
    "        binary_acc = accuracy_score(binary_true, binary_pred)\n",
    "        if len(np.unique(binary_pred)) == 1: binary_f1 = 0.0; print(\"Warning: Model predicts all samples as same class\")\n",
    "        else: binary_f1 = f1_score(binary_true, binary_pred)\n",
    "        print(f\"\\n=== Regression Evaluation ===\"); print(f\"MAE: {mae:.2f}\"); print(f\"RMSE: {rmse:.2f}\"); print(f\"R²: {r2:.3f}\")\n",
    "        print(f\"Predictions >= 10: {(predictions >= 10).sum()} out of {len(predictions)}\")\n",
    "        print(f\"\\nBinary Classification (PHQ-8 >= 10):\"); print(f\"Accuracy: {binary_acc:.3f}\"); print(f\"F1-Score: {binary_f1:.3f}\")\n",
    "        cm = confusion_matrix(binary_true, binary_pred)\n",
    "        print(f\"\\nConfusion Matrix:\\nTN: {cm[0,0]}, FP: {cm[0,1]}\\nFN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "        return predictions, true_values, mae\n",
    "\n",
    "    def local_plot_depression_results(predictions, true_values):\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes[0, 0].scatter(true_values, predictions, alpha=0.5); axes[0, 0].plot([0, 24], [0, 24], 'r--')\n",
    "        axes[0, 0].axhline(y=10, color='green', linestyle='--', alpha=0.5); axes[0, 0].axvline(x=10, color='green', linestyle='--', alpha=0.5)\n",
    "        axes[0, 0].set_xlabel('True PHQ-8'); axes[0, 0].set_ylabel('Predicted PHQ-8'); axes[0, 0].set_title('Predictions vs True Values')\n",
    "        binary_true = (true_values >= 10).astype(int); binary_pred = (predictions >= 10).astype(int)\n",
    "        cm = confusion_matrix(binary_true, binary_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1]); axes[0, 1].set_xlabel('Predicted'); axes[0, 1].set_ylabel('True'); axes[0, 1].set_title('Depression Detection (PHQ-8 >= 10)')\n",
    "        axes[1, 0].hist(predictions[binary_true == 0], bins=15, alpha=0.5, label='Not Depressed', color='blue')\n",
    "        axes[1, 0].hist(predictions[binary_true == 1], bins=15, alpha=0.5, label='Depressed', color='red')\n",
    "        axes[1, 0].axvline(x=10, color='black', linestyle='--', label='Threshold'); axes[1, 0].set_xlabel('Predicted PHQ-8'); axes[1, 0].set_ylabel('Count'); axes[1, 0].legend(); axes[1, 0].set_title('Prediction Distribution by True Class')\n",
    "        thresholds = np.linspace(0, 20, 100); tpr, fpr = [], []\n",
    "        for thresh in thresholds:\n",
    "            pred_binary_roc = (predictions >= thresh).astype(int)\n",
    "            tp=((pred_binary_roc==1)&(binary_true==1)).sum(); fp=((pred_binary_roc==1)&(binary_true==0)).sum()\n",
    "            tn=((pred_binary_roc==0)&(binary_true==0)).sum(); fn=((pred_binary_roc==0)&(binary_true==1)).sum()\n",
    "            tpr.append(tp/(tp+fn) if (tp+fn)>0 else 0); fpr.append(fp/(fp+tn) if (fp+tn)>0 else 0)\n",
    "        axes[1, 1].plot(fpr, tpr); axes[1, 1].plot([0, 1], [0, 1], 'r--'); axes[1, 1].set_xlabel('False Positive Rate'); axes[1, 1].set_ylabel('True Positive Rate'); axes[1, 1].set_title('ROC-like Curve')\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- Main Execution Logic ---\n",
    "    try:\n",
    "        print(\"Loading saved models...\")\n",
    "        model_emotion = LocalSpeechEmotionModel(num_classes=NUM_CLASSES, input_feature_dim=cfg.N_MFCC_DEP).to(DEVICE)\n",
    "        model_emotion.load_state_dict(torch.load(cfg.DEPRESSION_PRETRAINED_MODEL_PATH, map_location=DEVICE))\n",
    "        model_emotion.eval()\n",
    "        \n",
    "        df_daic_woz = local_load_daic_woz_data(cfg.DAIC_WOZ_DATA_PATH)\n",
    "        print(f\"Loaded {len(df_daic_woz)} DAIC-WOZ samples\")\n",
    "\n",
    "        depression_dataset = LocalSpeechAudioDataset(df_daic_woz, feature_type='mfcc', n_features=cfg.N_MFCC_DEP)\n",
    "        depression_loader = DataLoader(depression_dataset, batch_size=cfg.BATCH_SIZE_DEP, shuffle=False, collate_fn=local_pad_collate)\n",
    "\n",
    "        print(\"Extracting emotion features...\")\n",
    "        depression_emotions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(depression_loader, desc=\"Extracting\"):\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model_emotion(inputs)\n",
    "                depression_emotions.append({'values': outputs.cpu(), 'phq8': labels.cpu()})\n",
    "\n",
    "        rows = []\n",
    "        for item in depression_emotions:\n",
    "            values, phq8 = item['values'], item['phq8']\n",
    "            for v, p in zip(values, phq8):\n",
    "                rows.append({'values': v.tolist(), 'phq8': p.item()})\n",
    "        df_features = pd.DataFrame(rows)\n",
    "        print(f\"Created feature DataFrame with {len(df_features)} samples\")\n",
    "\n",
    "        print(\"\\nAugmenting depression cases...\")\n",
    "        depressed_df = df_features[df_features['phq8'] >= 10].copy()\n",
    "        augmented_rows = []\n",
    "        for _ in range(3):\n",
    "            for _, row in depressed_df.iterrows():\n",
    "                noise = np.random.normal(0, 0.02, size=len(row['values']))\n",
    "                augmented_values = np.clip(np.array(row['values']) + noise, 0, 1).tolist()\n",
    "                augmented_rows.append({'values': augmented_values, 'phq8': row['phq8'] + np.random.normal(0, 0.5)})\n",
    "        \n",
    "        df_augmented = pd.DataFrame(augmented_rows)\n",
    "        df_train_full = pd.concat([df_features, df_augmented], ignore_index=True)\n",
    "        print(f\"Augmented to {len(df_train_full)} samples\")\n",
    "        print(f\"Depression cases: {(df_train_full['phq8'] >= 10).sum()} ({(df_train_full['phq8'] >= 10).mean()*100:.1f}%)\")\n",
    "        \n",
    "        train_df, test_df = train_test_split(df_train_full, test_size=0.2, random_state=42, stratify=(df_train_full['phq8'] >= 10))\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "        print(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "        train_loader = DataLoader(LocalDepressionDataset(train_df), batch_size=cfg.BATCH_SIZE_DEP, shuffle=True)\n",
    "        val_loader = DataLoader(LocalDepressionDataset(val_df), batch_size=cfg.BATCH_SIZE_DEP, shuffle=False)\n",
    "        test_loader = DataLoader(LocalDepressionDataset(test_df), batch_size=cfg.BATCH_SIZE_DEP, shuffle=False)\n",
    "\n",
    "        n_inputs = len(df_features.iloc[0]['values'])\n",
    "        model_depression = LocalDepressionModel(n_inputs, hidden_dims=[64, 32]).to(DEVICE)\n",
    "        \n",
    "        mean_phq8 = df_train_full['phq8'].mean()\n",
    "        with torch.no_grad():\n",
    "            for name, param in model_depression.named_parameters():\n",
    "                if 'bias' in name and param.shape[0] == 1:\n",
    "                    param.data.fill_(mean_phq8)\n",
    "        \n",
    "        criterion = LocalWeightedMSELoss()\n",
    "        optimizer = torch.optim.Adam(model_depression.parameters(), lr=cfg.LEARNING_RATE_DEP)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "        print(f\"\\nTraining for {cfg.NUM_EPOCHS_DEP} epochs...\")\n",
    "        best_val_f1, best_val_mae = 0.0, float('inf')\n",
    "\n",
    "        for epoch in range(cfg.NUM_EPOCHS_DEP):\n",
    "            train_loss = local_train_model_epoch(model_depression, train_loader, criterion, optimizer, DEVICE, epoch, cfg.NUM_EPOCHS_DEP)\n",
    "            \n",
    "            val_predictions, val_true, val_mae = local_evaluate_model_regression(model_depression, val_loader, DEVICE)\n",
    "            val_binary_pred = (val_predictions >= 10).astype(int)\n",
    "            val_binary_true = (val_true >= 10).astype(int)\n",
    "            val_f1 = f1_score(val_binary_true, val_binary_pred, zero_division=0)\n",
    "            scheduler.step(val_mae)\n",
    "            \n",
    "            if val_f1 > best_val_f1 or (val_f1 == best_val_f1 and val_mae < best_val_mae):\n",
    "                best_val_mae, best_val_f1 = val_mae, val_f1\n",
    "                torch.save(model_depression.state_dict(), 'best_depression_model.pth')\n",
    "                print(f\"New best model! F1: {val_f1:.3f}, MAE: {val_mae:.2f}\")\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"\\nEpoch [{epoch+1}/{cfg.NUM_EPOCHS_DEP}]\")\n",
    "                print(f\"Train Loss: {train_loss:.4f}, Val MAE: {val_mae:.2f}, Val F1: {val_f1:.3f}\")\n",
    "                print(f\"Predicting {(val_predictions >= 10).sum()}/{len(val_predictions)} as depressed\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\nFINAL TEST EVALUATION\\n\" + \"=\"*50)\n",
    "        model_depression.load_state_dict(torch.load('best_depression_model.pth'))\n",
    "        test_predictions, test_true, _ = local_evaluate_model_regression(model_depression, test_loader, DEVICE)\n",
    "        \n",
    "        if (test_predictions >= 10).sum() == 0:\n",
    "            print(\"\\nModel still too conservative. Applying post-hoc adjustment...\")\n",
    "            n_force = max(2, int(0.2 * len(test_predictions)))\n",
    "            top_indices = np.argsort(test_predictions)[-n_force:]\n",
    "            adjusted_predictions = test_predictions.copy()\n",
    "            for idx in top_indices:\n",
    "                adjusted_predictions[idx] = 10 + (test_predictions[idx] - test_predictions.min()) * 2\n",
    "            \n",
    "            binary_true_adj = (test_true >= 10).astype(int)\n",
    "            binary_pred_adj = (adjusted_predictions >= 10).astype(int)\n",
    "            print(f\"\\nAfter adjustment:\\nPredictions >= 10: {binary_pred_adj.sum()}\")\n",
    "            print(f\"F1-Score: {f1_score(binary_true_adj, binary_pred_adj):.3f}\")\n",
    "            test_predictions = adjusted_predictions\n",
    "            \n",
    "        local_plot_depression_results(test_predictions, test_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\nAn error occurred during the depression detection part: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\nDepression Detection (Perfect Replication) Complete.\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbef748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
